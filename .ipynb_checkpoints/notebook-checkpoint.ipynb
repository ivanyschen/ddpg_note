{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- randomly initialize critic network `Q(s, a)` and actor `Mu(s)` network with `W_Q` and `W_Mu`  \n",
    "- initialize target networks `Q'(s, a)` and `Mu'(s)` with weights `W_Q' = W_Q` and `W_Mu' = W_Mu`  \n",
    "- initialize replay buffer `R`\n",
    "\n",
    "- for iteration 1 to M:  \n",
    "  - initialize a random process for **action exploration**\n",
    "  - receive initial observation state `s1`\n",
    "  \n",
    "  - for t 1 to T:\n",
    "      - select action `a_t = Mu(s_t) + N_t` according to the current policy and exploration noise\n",
    "      - execute action `a_t` and observe reward `r_t` and observe new state s_{t+1}\n",
    "      - store transition (s_t, a_t, r_t, s_{t+1}) in `R`\n",
    "      - sample a random minibatch of N transitions (s_i, a_i, r_i, s_{i+1}) from `R`\n",
    "      - set `y_i = r_i + gamma * Q'(s_{i+1}, Mu'(s_{i+1}))`\n",
    "      - update critic by minimizing the loss: `L = (1/N) * sum_{(y_i - Q(s_i, a_i))^2}`\n",
    "      - update the actor policy using the sampled policy gradient: del J = (1/N) sum_{del_{a} Q(s, a)|{s=s, a=Mu(si)} * del M}\n",
    "      ![](policy_gradient.gif)  \n",
    "      - update the target network `W_Q' = tau * W_Q + (1 - tao) * W_Q'` and `W_Mu' = tau * W_Mu + (1 - tau) * W_Mu'`\n",
    "      \n",
    "By Lillicrap et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "class Actor:\n",
    "    \n",
    "    def __init__(self, tf_session, action_shape, hidden_layer_units, \n",
    "                 learning_rate, batch_size, tau):\n",
    "        \n",
    "        self._tf_session = tf_session\n",
    "        self._action_shape = action_shape    # the shape of states\n",
    "        self._hidden_layer_units = hidden_layer_units\n",
    "        self._learning_rate = learning_rate\n",
    "        self._batch_size = batch_size\n",
    "        self._tau = tau\n",
    "        \n",
    "        K.set_session(self._tf_session)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _generate_network(self):\n",
    "        input_layer = None\n",
    "        dense = Dense(unit=self._hidden_layer_units[0])(input_layer)\n",
    "        for unit in self._hidden_layer_units[1:]:\n",
    "            dense = Dense(unit=unit)(dense)\n",
    "        output_layer = Dense(unit=self._action_shape, activation='sigmoid')\n",
    "    \n",
    "    def train_main_network(self):\n",
    "        pass\n",
    "    \n",
    "    def train_target_network(self):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
