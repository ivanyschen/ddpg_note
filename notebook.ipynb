{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- randomly initialize critic network `Q(s, a)` and actor `Mu(s)` network with `W_Q` and `W_Mu`  \n",
    "- initialize target networks `Q'(s, a)` and `Mu'(s)` with weights `W_Q' = W_Q` and `W_Mu' = W_Mu`  \n",
    "- initialize replay buffer `R`\n",
    "\n",
    "- for iteration 1 to M:  \n",
    "  - initialize a random process for **action exploration**\n",
    "  - receive initial observation state `s1`\n",
    "  \n",
    "  - for t 1 to T:\n",
    "      - select action `a_t = Mu(s_t) + N_t` according to the current policy and exploration noise\n",
    "      - execute action `a_t` and observe reward `r_t` and observe new state s_{t+1}\n",
    "      - store transition (s_t, a_t, r_t, s_{t+1}) in `R`\n",
    "      - sample a random minibatch of N transitions (s_i, a_i, r_i, s_{i+1}) from `R`\n",
    "      - set `y_i = r_i + gamma * Q'(s_{i+1}, Mu'(s_{i+1}))`\n",
    "      - update critic by minimizing the loss: `L = (1/N) * sum_{(y_i - Q(s_i, a_i))^2}`\n",
    "      - update the actor policy using the sampled policy gradient:  \n",
    "      ![](policy_gradient.gif)  \n",
    "      - update the target network `W_Q' = tau * W_Q + (1 - tao) * W_Q'` and `W_Mu' = tau * W_Mu + (1 - tau) * W_Mu'`\n",
    "      \n",
    "By Lillicrap et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Actor:\n",
    "    \n",
    "    def __init__(self, tf_session, state_size, action_size, hidden_layer_units=(300, 600), \n",
    "                 learning_rate=0.0001, batch_size=64, tau=0.001):\n",
    "        \n",
    "        self._tf_session = tf_session\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size    # the shape of states\n",
    "        self._hidden_layer_units = hidden_layer_units\n",
    "        self._batch_size = batch_size\n",
    "        self._tau = tau\n",
    "        \n",
    "        K.set_session(self._tf_session)\n",
    "        \n",
    "        self._main_model = self._generate_model()\n",
    "        self._target_model = self._generate_model()\n",
    "        \n",
    "        self._critic_gradients = tf.placeholder(shape=(action_size, ), dtype=tf.float32, name='critic_grad')  #Del_{a} Q(s, a)\n",
    "        self._actor_gradients = tf.gradients(self._main_model.output,\n",
    "                                             self._main_model.trainable_weights,\n",
    "                                             -self._critic_gradients)\n",
    "        \n",
    "        self._optimize = tf.train.AdamOptimizer(learning_rate).\\\n",
    "                apply_gradients(zip(self._actor_gradients, self._main_model.weights))\n",
    "        \n",
    "        self._tf_session.run(tf.initializers.global_variables())\n",
    "        \n",
    "    def _generate_model(self):\n",
    "        input_ = Input(shape=(self._state_size, ), name='state')\n",
    "        dense = Dense(units=self._hidden_layer_units[0])(input_)\n",
    "        for unit in self._hidden_layer_units[1:]:\n",
    "            dense = Dense(units=unit)(dense)\n",
    "        output_layer = Dense(units=self._action_size, activation='sigmoid')(dense)\n",
    "        \n",
    "        model = Model(inputs=input_, outputs=output_layer)\n",
    "        return model\n",
    "    \n",
    "    def train_main_model(self, states, critic_gradients):\n",
    "        self._tf_session.run(self._optimize, feed_dict={\n",
    "            'state': states,\n",
    "            'critic_grad': critic_gradients\n",
    "        })\n",
    "    \n",
    "    def train_target_model(self):\n",
    "        weights_main = self._main_model.get_weights()\n",
    "        weights_target = self._target_model.get_weights()\n",
    "        new_target_weights = [self._tau * m + (1 - self._tau) * t for m, t in zip(weights_main, weights_target)]\n",
    "        self._target_model.set_weights(new_target_weights)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        return self._main_model.predict(state.reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "actor = Actor(sess, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- `apply_gradients()`: Apply gradients to variables. This is the second part of minimize(). It returns an Operation that applies gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    \n",
    "    def __init__(self, tf_session, state_size, action_size, hidden_layer_units=(300, 600),\n",
    "                 learning_rate=0.0001, batch_size=64, tau=0.001):\n",
    "        \n",
    "        self._tf_session = tf_session\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size\n",
    "        self._hidden_layer_units = hidden_layer_units\n",
    "        self._learning_rate = learning_rate\n",
    "        self._tau = tau\n",
    "        \n",
    "        K.set_session(self._tf_session)\n",
    "        \n",
    "        self._main_model, self._action_input = self._generate_model()\n",
    "        self._target_model, _ = self._generate_model()\n",
    "        \n",
    "        self._tf_session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "    def _generate_model(self):\n",
    "        state_input = Input(shape=(self._state_size, ), name='state')\n",
    "        action_input = Input(shape=(self._action_size, ), name='action')  \n",
    "        total_input = Concatenate(axis=1)([state_input, action_input])\n",
    "        dense = Dense(units=self._hidden_layer_units[0], activation='relu')(total_input)\n",
    "        for unit in self._hidden_layer_units[1:]:\n",
    "            dense = Dense(units=unit, activation='relu')(dense)\n",
    "        output = Dense(units=1, activation='relu')(dense)\n",
    "        model = Model(inputs=[state_input, action_input], outputs=output)\n",
    "        \n",
    "#         Y = K.placeholder(shape=(1, ), name='Y')\n",
    "#         self._loss = tf.losses.mean_squared_error(Y, self._main_model.output)\n",
    "#         optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "        \n",
    "        return action_input, model\n",
    "    \n",
    "    def compute_gradients_wrt_actions(self, states, actions):\n",
    "        gradients_wrt_actions_op = tf.gradients(self._main_model.output, self._action_input)\n",
    "        return self._tf_seesion.run(gradients_wrt_actions_op, feed_dict={'state': states, 'action': actions})[0]\n",
    "    \n",
    "    def train_main_model(self, states, actions, Y):\n",
    "        Y = K.placeholder(shape=(1, ), name='Y')\n",
    "        self._loss = tf.losses.mean_squared_error(Y, self._main_model.output)\n",
    "        optimizer = tf.train.AdamOptimizer(self._learning_rate)        \n",
    "\n",
    "        self._tf_session.run(optimizer.minimize(self._loss),\n",
    "            feed_dict={'state': states, 'action': actions, 'Y': Y})\n",
    "        \n",
    "    def train_target_model(self):\n",
    "        weights_main = self._main_model.get_weights()\n",
    "        weights_target = self._target_model.get_weights()\n",
    "        new_weights = [self._tau * m + (1 - self._tau) * t for m, t in zip(weights_main, weights_target)]\n",
    "        weights_target.set_weights(new_weights)\n",
    "        \n",
    "    def get_next_state(self, state, action):\n",
    "        return self._main_model.predict(zip(state.reshape(1, -1), action.reshape(1, -1)))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, initial_state):\n",
    "        self.initial_state = initial_state\n",
    "        \n",
    "    def get_reward_and_next_state(self, state, action):\n",
    "        reward = np.random.rand\n",
    "        next_state = np.random.rand(*state.shape)\n",
    "        return reward, next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DDPG:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, actor_hidden_units=(300, 600), actor_learning_rate=0.0001,\n",
    "                 critic_hidden_units=(300, 600), critic_learning_rate=0.0001, batch_size=64,\n",
    "                 reward_discount=0.99, memory_size=100000, tau=0.001, T=1000, minibatch_size=64, gamma=0.001):\n",
    "        \n",
    "        self._action_size = action_size\n",
    "        self._t = 0\n",
    "        self._T = T\n",
    "        self._minibatch_size = minibatch_size\n",
    "        self._gamma = gamma\n",
    "        \n",
    "        tf_session = tf.Session()\n",
    "        \n",
    "        self._actor = Actor(tf_session=tf_session,\n",
    "                            state_size=state_size,\n",
    "                            action_size=action_size,\n",
    "                            hidden_layer_units=actor_hidden_units,\n",
    "                            learning_rate=actor_learning_rate,\n",
    "                            batch_size=batch_size,\n",
    "                            tau=tau)\n",
    "        \n",
    "        self._critic = Critic(tf_session=tf_session,\n",
    "                              state_size=state_size,\n",
    "                              action_size=action_size,\n",
    "                              hidden_layer_units=critic_hidden_units,\n",
    "                              learning_rate=critic_learning_rate,\n",
    "                              batch_size=batch_size,\n",
    "                              tau=tau)\n",
    "        \n",
    "        self._R = []\n",
    "        self._R_size = 0\n",
    "        self._memory_size = memory_size\n",
    "        \n",
    "        self.random_actions = self._initialize_random_actions()\n",
    "        \n",
    "    def _initialize_random_actions(self):\n",
    "        return np.random.rand(*(self._T, ) + (self._action_size, ))\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        return self._actor.select_action(state) + self.random_actions[self._t]\n",
    "    \n",
    "    def go_to_next_state(self, state, action):\n",
    "        return self._critic.get_next_state(state, action)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, state_next):\n",
    "        self._R.append((state, action, reward, state_next))\n",
    "        self._R_size += 1\n",
    "        if self._memory_size < self._R_size:\n",
    "            self.R.pop(0)\n",
    "            self._R_size -= 1\n",
    "    \n",
    "    def sample_minibatch_transition(self):\n",
    "        samples = random.sample(self._R, self._minibatch_size)\n",
    "        return zip(*samples)\n",
    "    \n",
    "    def compute_Y(self, states, actions, rewards, state_nexts):\n",
    "        states = np.vstack(states)\n",
    "        actions = np.vstack(actions)\n",
    "        rewards = np.vstack(rewards)\n",
    "        state_nexts = np.vstack(state_nexts)\n",
    "        \n",
    "        action_nexts = self._actor.select_action(state_nexts)\n",
    "        Y = R + self._critic.get_next_state(state_nexts, action_nexts) * self._gamma\n",
    "        return Y\n",
    "    \n",
    "    def train_critic(self, states, actions, Y):\n",
    "        self._critic.train_main_model(states, actions, Y)\n",
    "        \n",
    "    def compute_critic_gradients(self, states, actions):\n",
    "        return self._critic.compute_gradients_wrt_actions(states, actions)\n",
    "    \n",
    "    def train_actor(self, states, critic_gradients):\n",
    "        self._actor.train_main_model(self, states, critic_gradients)\n",
    "        \n",
    "    def update_target_models(self):\n",
    "        self._critic.train_target_model()\n",
    "        self._target.train_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100\n",
    "T = 100\n",
    "env = Environment(np.random.rand(1, 3))\n",
    "agent = DDPG(state_size=3, action_size=5)\n",
    "\n",
    "for _ in range(M):\n",
    "    init_state = env.initial_state\n",
    "    for _ in range(T):\n",
    "        action = agent.select_action(init_state)\n",
    "        reward, next_state = env.get_reward_and_next_state(init_state, action)\n",
    "        agent.store_transition(init_state, action, reward, next_state)\n",
    "        \n",
    "        states, actions, rewards, state_nexts = agent.sample_minibatch_transition()\n",
    "        Y = agent.compute_Y(states, actions, rewards, state_nexts)\n",
    "        agent.train_critic(states, actions, Y)\n",
    "        critic_gradients = agent.compute_critic_gradients(states, actions)\n",
    "        agent.train_actor(states, gradients)\n",
    "        agent.update_target_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
