{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- randomly initialize critic network `Q(s, a)` and actor `Mu(s)` network with `W_Q` and `W_Mu`  \n",
    "- initialize target networks `Q'(s, a)` and `Mu'(s)` with weights `W_Q' = W_Q` and `W_Mu' = W_Mu`  \n",
    "- initialize replay buffer `R`\n",
    "\n",
    "- for iteration 1 to M:  \n",
    "  - initialize a random process for **action exploration**\n",
    "  - receive initial observation state `s1`\n",
    "  \n",
    "  - for t 1 to T:\n",
    "      - select action `a_t = Mu(s_t) + N_t` according to the current policy and exploration noise\n",
    "      - execute action `a_t` and observe reward `r_t` and observe new state s_{t+1}\n",
    "      - store transition (s_t, a_t, r_t, s_{t+1}) in `R`\n",
    "      - sample a random minibatch of N transitions (s_i, a_i, r_i, s_{i+1}) from `R`\n",
    "      - set `y_i = r_i + gamma * Q'(s_{i+1}, Mu'(s_{i+1}))`\n",
    "      - update critic by minimizing the loss: `L = (1/N) * sum_{(y_i - Q(s_i, a_i))^2}`\n",
    "      - update the actor policy using the sampled policy gradient:  \n",
    "      ![](policy_gradient.gif)  \n",
    "      - update the target network `W_Q' = tau * W_Q + (1 - tao) * W_Q'` and `W_Mu' = tau * W_Mu + (1 - tau) * W_Mu'`\n",
    "      \n",
    "By Lillicrap et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Actor:\n",
    "    \n",
    "    def __init__(self, tf_session, state_size, action_size, hidden_layer_units=(300, 600), \n",
    "                 learning_rate=0.0001, batch_size=64, tau=0.001):\n",
    "        \n",
    "        self._tf_session = tf_session\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size    # the shape of states\n",
    "        self._hidden_layer_units = hidden_layer_units\n",
    "        self._batch_size = batch_size\n",
    "        self._tau = tau\n",
    "        \n",
    "        K.set_session(self._tf_session)\n",
    "        \n",
    "        self._main_model = self._generate_model()\n",
    "        self._target_model = self._generate_model()\n",
    "        \n",
    "        self._critic_gradients = tf.placeholder(shape=(action_size, None))  #Del_{a} Q(s, a)\n",
    "        self._actor_gradients = tf.gradients(self._main_model.output,\n",
    "                                             self._main_model.trainable_weights,\n",
    "                                             -self._critics_gradients)\n",
    "        self._gradients = zip(self._actor_gradients, self._main_model.weights)\n",
    "        \n",
    "        self._optimize = tf.train.AdamOptimizer(learning_rate).apply_gradients(self._gradients)\n",
    "        \n",
    "        self._tf_session.run(tf.initialize_all_variables())\n",
    "        \n",
    "    def _generate_model(self):\n",
    "        self._state = Input(shape=self._state_size)\n",
    "        dense = Dense(unit=self._hidden_layer_units[0])(self._state)\n",
    "        for unit in self._hidden_layer_units[1:]:\n",
    "            dense = Dense(unit=unit)(dense)\n",
    "        output_layer = Dense(unit=self._action_size, activation='sigmoid')\n",
    "        \n",
    "        model = Model(inputs=self._state, outputs=output_layer)\n",
    "        return model\n",
    "    \n",
    "    def train_main_model(self, states, critic_gradients):\n",
    "        self._tf_session.run(self._optimize, feed_dict={\n",
    "            self._state: states,\n",
    "            self._critic_gradients: critic_gradients\n",
    "        })\n",
    "    \n",
    "    def train_target_model(self):\n",
    "        weights_main = self._main_model.get_weights()\n",
    "        weights_target = self._target_model.get_weights()\n",
    "        new_target_weights = [self._tau * m + (1 - self._tau) * t for m, t in zip(weights_main, weights_target)]\n",
    "        self._target_model.set_weights(new_target_weights)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- `apply_gradients()`: Apply gradients to variables. This is the second part of minimize(). It returns an Operation that applies gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    \n",
    "    def __init__(self, tf_session, state_size, action_size, hidden_layer_units=(300, 600),\n",
    "                 learning_rate=0.0001, batch_size=64, tau=0.001):\n",
    "        \n",
    "        self._tf_session = tf_session\n",
    "        self._state_size = state_size\n",
    "        self._action_size = action_size\n",
    "        self._hidden_layer_units = hidden_layer_units\n",
    "        self._learning_rate = learning_rate\n",
    "        self._tau = tau\n",
    "        \n",
    "        K.set_session(self._tf_session)\n",
    "        \n",
    "        self._main_model = self._generate_model()\n",
    "        self._target_model = self._generate_model()\n",
    "        \n",
    "        loss = tf.losses.mean_squared_error(self.Y, self._main_model.output)\n",
    "        self._compute\n",
    "        self._optimize = .apply_gradients(self._gradients)\n",
    "        \n",
    "        self._tf_session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        \n",
    "    def _generate_model(self):\n",
    "        state_input = Input(shape=self._state_size, name='state')\n",
    "        action_input = Input(shape=self._action_size, name='action')  \n",
    "        total_input = Concatenate(axis=1)([state_input, action_input])\n",
    "        dense = Dense(unit=self._hidden_layer_units[0], activation='relu')(total_input)\n",
    "        for unit in self._hidden_layer_units[1:]:\n",
    "            dense = Dense(unit=unit, activation='relu')(dense)\n",
    "        output = Dense(unit=1, activation='relu')\n",
    "        model = Model(inputs=[self._state, self._action], outputs=output)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train_main_model(self, states, actions, Yi):\n",
    "        Y = K.placeholder(shape=(1, None), name='Y')\n",
    "        loss = tf.losses.mean_squared_error(Y, self._main_model.output)\n",
    "        optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "        gradients = self._tf_session.run(optimizer.compute_gradients(loss),\n",
    "                                         feed_dict={'state': states, 'action': actions})\n",
    "        self._tf_session.run(optimizer.apply_gradients(gradients))\n",
    "        return gradients[0]\n",
    "        \n",
    "        \n",
    "    def train_target_model(self):\n",
    "        weights_main = self._main_model.get_weights()\n",
    "        weights_target = self._target_model.get_weights()\n",
    "        new_weights = [self._tau * m + (1 - self._tau) * t for m, t in zip(weights_main, weights_target)]\n",
    "        weights_target.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class DDPG:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, actor_hidden_units=(300, 600), actor_learning_rate=0.0001,\n",
    "                 critic_hidden_units=(300, 600), critic_learning_rate=0.0001, batch_size=64,\n",
    "                 reward_discount=0.99, memory_size=100000, tau=0.001, T=1000, sampling_size=64, gamma=0.001):\n",
    "        \n",
    "        self._t = 0\n",
    "        self._T = T\n",
    "        self._sampling_size = sampling_size\n",
    "        self._gamma = gamma\n",
    "        \n",
    "        tf_session = tf.Session()\n",
    "        \n",
    "        self._actor = Actor(tf_session=tf_session,\n",
    "                            state_size=state_size,\n",
    "                            action_size=action_size,\n",
    "                            hidden_layer_units=actor_hidden_units,\n",
    "                            learning_rate=actor_learning_rate,\n",
    "                            batch_size=batch_size,\n",
    "                            tau=tau)\n",
    "        \n",
    "        self._critic = Critic(tf_session=tf_session,\n",
    "                              state_size=state_size,\n",
    "                              action_size=action_size,\n",
    "                              hidden_layer_units=critic_hidden_units,\n",
    "                              learning_rate=critic_learning_rate,\n",
    "                              batch_size=batch_size,\n",
    "                              tau=tau)\n",
    "        \n",
    "        self._R = []\n",
    "        self._R_size = 0\n",
    "        self._memory_size = memory_size\n",
    "        \n",
    "        self.random_actions = self._get_random_actions()\n",
    "        \n",
    "    def _get_random_actions(self):\n",
    "        return np.random.rand(*(self._T, ) + action_size)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        return self._actor._model.predict(state) + self.random_actions[self._t]\n",
    "    \n",
    "    def remember(self, s_t, a_t, r_t, s_tplusone):\n",
    "        self._R.append((s_t, a_t, r_t, s_tplusone))\n",
    "        self._R_size += 1\n",
    "        if self._memory_size < self._R_size:\n",
    "            self.R.pop(0)\n",
    "            self._R_size -= 1\n",
    "    \n",
    "    def sample_transitions(self):\n",
    "        samples = random.sample(self._R, self._sampling_size)\n",
    "        return zip(*samples)\n",
    "    \n",
    "    def compute_Yi(self, samples):\n",
    "        s_i, a_i, r_i, s_iplusone = samples\n",
    "        s_i = np.array(s_i)\n",
    "        a_i = np.array(a_i)\n",
    "        r_i = np.array(r_i)\n",
    "        s_iplusone = np.array(s_iplusone)\n",
    "        \n",
    "        a_i_pred = self._actor._target_model.predict(s_iplusone)\n",
    "        y_i = r_i + self._critic._target_model.predict(s_iplusone, a_i_pred) * self._gamma\n",
    "        return s_i, a_i, y_i\n",
    "    \n",
    "    def train_and_get_q(self, state, action, y_i):\n",
    "        return self._critic._main_model.train_main_model(state, a_i, y_i)\n",
    "    \n",
    "    def update_main_actor(self, states, critic_gradients):\n",
    "        self._actor._main_model.train_main_model(states, critic_gradients)\n",
    "        \n",
    "    def update_target_models(self):\n",
    "        self._critic.train_target_model()\n",
    "        self._target.train_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
